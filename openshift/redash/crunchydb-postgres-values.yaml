fullnameOverride: crunchy-postgres-redash

crunchyImage: # it's not necessary to specify an image as the images specified in the Crunchy Postgres Operator will be pulled by default
#crunchyImage: artifacts.developer.gov.bc.ca/bcgov-docker-local/crunchy-postgres-gis:ubi8-15.2-3.3-0 # use this image for POSTGIS
postgresVersion: 15
#postGISVersion: '3.3' # use this version of POSTGIS. both crunchyImage and this property needs to have valid values for POSTGIS to be enabled.
imagePullPolicy: IfNotPresent

# enable to bootstrap a standby cluster from backup. Then disable to promote this standby to primary
standby:
  enabled: false
  # If you want to recover from PVC, use repo1. If you want to recover from S3, use repo1
  repoName: repo1

instances:
  name: ha # high availability
  replicas: 2
  dataVolumeClaimSpec:
    storage: 480Mi
    storageClassName: netapp-block-standard
  requests:
    cpu: 1m
    memory: 256Mi
  limits:
    cpu: 100m
    memory: 512Mi
  replicaCertCopy:
    requests:
      cpu: 1m
      memory: 32Mi
    limits:
      cpu: 50m
      memory: 64Mi

# If we need to restore the cluster from a backup, we need to set the following values
# assuming restore from repo1 (s3), adjust as needed if your S3 repo is different
dataSource:
  enabled: false
  # should have the same name and contain the same keys as the pgbackrest secret
  secretName: s3-pgbackrest
  repo:
    name: repo1
    path: "/backups/redash"
    s3:
      bucket: "bucketName"
      endpoint: "s3.ca-central-1.amazonaws.com"
      region: "ca-central-1"
    stanza: db

pgBackRestConfig:
  image: # it's not necessary to specify an image as the images specified in the Crunchy Postgres Operator will be pulled by default
  repos:
    - name: repo1
      path: "/backups/redash"
      volume:
       accessModes:
         - "ReadWriteOnce"
       storage: 2Gi
       storageClassName: netapp-file-backup
      schedules:
        - name: full-weekly
          type: full
          schedule: '0 0 * * 0'
          retentionLimit: "12"
          retentionPolicy: count
        - name: diff-daily
          type: differential
          schedule: '0 0 * * *'
          retentionLimit: "6"
        - name: incremental
          type: incremental
          schedule: '0 0,4,8,12,16,20 * * *'
  repoHost:
    requests:
      cpu: 1m
      memory: 64Mi
    limits:
      cpu: 50m
      memory: 128Mi
  sidecars:
    requests:
      cpu: 1m
      memory: 64Mi
    limits:
      cpu: 50m
      memory: 128Mi

patroni:
  postgresql:
    pg_hba: "host all all 0.0.0.0/0 md5"
    parameters:
      shared_buffers: 16MB # default is 128MB; a good tuned default for shared_buffers is 25% of the memory allocated to the pod
      wal_buffers: "64kB" # this can be set to -1 to automatically set as 1/32 of shared_buffers or 64kB, whichever is larger
      min_wal_size: 32MB
      max_wal_size: 64MB # default is 1GB
      max_slot_wal_keep_size: 128MB # default is -1, allowing unlimited wal growth when replicas fall behind

proxy:
  pgBouncer:
    image: # it's not necessary to specify an image as the images specified in the Crunchy Postgres Operator will be pulled by default
    replicas: 2
    requests:
      cpu: 1m
      memory: 64Mi
    limits:
      cpu: 50m
      memory: 128Mi

# Postgres Cluster resource values:
pgmonitor:
  enabled: false
  exporter:
    image: # it's not necessary to specify an image as the images specified in the Crunchy Postgres Operator will be pulled by default
    requests:
      cpu: 1m
      memory: 64Mi
    limits:
      cpu: 50m
      memory: 128Mi
